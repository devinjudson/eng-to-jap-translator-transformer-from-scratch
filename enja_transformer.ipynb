{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16e5e47",
   "metadata": {},
   "source": [
    "\n",
    "# English → Japanese Transformer from Scratch\n",
    "\n",
    "This notebook trains a small sequence-to-sequence **Transformer** to translate **English → Japanese** from a large database of japanese sentences and their respective english translations.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads & cleans a .TSV file.\n",
    "2. Trains a shared **SentencePiece** tokenizer suited for Japanese.\n",
    "3. Builds a PyTorch **Transformer** using `nn.Transformer` for seq2seq.\n",
    "4. Trains with teacher forcing, validates each epoch, and reports **BLEU**.\n",
    "5. Saves artifacts (tokenizer + model) and provides a `translate()` helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally, uncomment to install dependencies.\n",
    "# !pip install --upgrade torch sentencepiece sacrebleu matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b5355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, io, csv, math, random, unicodedata, re, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============== SETTINGS ==============\n",
    "# Path to .TSV file\n",
    "DATA_TSV = \"Sentence pairs in Japanese-English - 2025-08-27.tsv\"\n",
    "\n",
    "# Training / model hyperparameters\n",
    "VOCAB_SIZE      = 16000\n",
    "MAX_LEN         = 128\n",
    "BATCH_SIZE      = 64\n",
    "EMBED_DIM       = 512\n",
    "FF_DIM          = 2048\n",
    "NHEAD           = 8\n",
    "ENC_LAYERS      = 4\n",
    "DEC_LAYERS      = 4\n",
    "DROPOUT         = 0.1\n",
    "EPOCHS          = 20\n",
    "LR              = 3e-4\n",
    "LABEL_SMOOTH    = 0.1\n",
    "\n",
    "VAL_FRACTION    = 0.05\n",
    "RANDOM_SEED     = 42\n",
    "\n",
    "# Output directory\n",
    "ARTIFACTS_DIR   = Path(\"artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determinism\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Print whether CPU or CUDA is being used\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f8c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 277,280 sentence pairs (after picking EN/JA columns).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def looks_japanese(s: str) -> bool:\n",
    "    return bool(re.search(r\"[\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff]\", s))\n",
    "\n",
    "def looks_english(s: str) -> bool:\n",
    "    return (bool(re.search(r\"[A-Za-z]\", s)) and not looks_japanese(s))\n",
    "\n",
    "def clean_text(s: str, lang: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def pick_text(cols, predicate):\n",
    "    best = \"\"\n",
    "    for c in cols:\n",
    "        c = c.strip()\n",
    "        if not c or c.isdigit():\n",
    "            continue\n",
    "        if predicate(c):\n",
    "            if len(c) > len(best):\n",
    "                best = c\n",
    "    return best\n",
    "\n",
    "def read_tsv_guess_columns(path: str) -> List[Tuple[str, str]]:\n",
    "    pairs = []\n",
    "    with io.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            if not row: \n",
    "                continue\n",
    "            # Clean each column\n",
    "            cols = [clean_text(x, \"en\") for x in row]\n",
    "\n",
    "            # Try to pick Japanese and English from any column\n",
    "            ja = pick_text(cols, looks_japanese)\n",
    "            en = pick_text(cols, looks_english)\n",
    "\n",
    "            # Fallback if detection is inconclusive\n",
    "            if not ja or not en:\n",
    "                if len(cols) >= 2:\n",
    "                    c1, c2 = cols[0], cols[1]\n",
    "                    if len(cols) >= 3:\n",
    "                        c3 = cols[2]\n",
    "                    else:\n",
    "                        c3 = \"\"\n",
    "                    if looks_japanese(c1) and (looks_english(c2) or looks_english(c3)):\n",
    "                        ja = ja or c1\n",
    "                        en = en or (c2 if looks_english(c2) else c3)\n",
    "                    elif looks_english(c1) and (looks_japanese(c2) or looks_japanese(c3)):\n",
    "                        en = en or c1\n",
    "                        ja = ja or (c2 if looks_japanese(c2) else c3)\n",
    "\n",
    "            if ja and en:\n",
    "                pairs.append((en, ja))\n",
    "    return pairs\n",
    "\n",
    "pairs = read_tsv_guess_columns(DATA_TSV)\n",
    "print(f\"Loaded {len(pairs):,} sentence pairs (after picking EN/JA columns).\")\n",
    "if not pairs:\n",
    "    raise SystemExit(\"No usable EN–JA pairs found; check the file format/path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2193162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 270,573 pairs\n",
      "Train: 257,045  |  Val: 13,528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic filters: drop duplicates, overly long pairs, extreme length ratios\n",
    "def tokenize_whitespace(s: str) -> List[str]:\n",
    "    return s.split()\n",
    "\n",
    "def filter_pairs(pairs, max_len=256, ratio=3.0):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for en, ja in pairs:\n",
    "        key = (en, ja)\n",
    "        if key in seen: \n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # crude length measures before subwording\n",
    "        en_len = len(tokenize_whitespace(en))\n",
    "        ja_len = len(ja)\n",
    "        if en_len == 0 or ja_len == 0:\n",
    "            continue\n",
    "        if en_len > max_len or ja_len > max_len*2:  # allow more chars for JA\n",
    "            continue\n",
    "        if max(en_len, ja_len) / max(1, min(en_len, ja_len)) > ratio:\n",
    "            continue\n",
    "        out.append((en, ja))\n",
    "    return out\n",
    "\n",
    "pairs = filter_pairs(pairs, max_len=MAX_LEN*2, ratio=4.0)\n",
    "print(f\"After filtering: {len(pairs):,} pairs\")\n",
    "\n",
    "# Train/val split\n",
    "random.shuffle(pairs)\n",
    "n_val = max(1, int(len(pairs) * VAL_FRACTION))\n",
    "val_pairs = pairs[:n_val]\n",
    "train_pairs = pairs[n_val:]\n",
    "print(f\"Train: {len(train_pairs):,}  |  Val: {len(val_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859277e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Write combined text file for SentencePiece\n",
    "spm_input_path = ARTIFACTS_DIR / \"spm_train.txt\"\n",
    "with io.open(spm_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for en, ja in train_pairs:\n",
    "        f.write(en + \"\\n\")\n",
    "        f.write(ja + \"\\n\")\n",
    "\n",
    "spm_prefix = str(ARTIFACTS_DIR / \"spm_enja\")\n",
    "spm_cmd = f\"--input={spm_input_path} --model_prefix={spm_prefix} --vocab_size={VOCAB_SIZE} \" \\\n",
    "          f\"--character_coverage=0.9995 --model_type=unigram \" \\\n",
    "          f\"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\"\n",
    "print(\"Training SentencePiece...\")\n",
    "spm.SentencePieceTrainer.Train(spm_cmd)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{spm_prefix}.model\")\n",
    "\n",
    "PAD_ID = sp.pad_id()\n",
    "UNK_ID = sp.unk_id()\n",
    "BOS_ID = sp.bos_id()\n",
    "EOS_ID = sp.eos_id()\n",
    "\n",
    "PAD_ID, UNK_ID, BOS_ID, EOS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3599b2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257045, 13528)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def encode_sentence(text: str, add_bos: bool, add_eos: bool) -> List[int]:\n",
    "    ids = sp.encode(text, out_type=int)\n",
    "    if add_bos:\n",
    "        ids = [BOS_ID] + ids\n",
    "    if add_eos:\n",
    "        ids = ids + [EOS_ID]\n",
    "    return ids\n",
    "\n",
    "def clip(ids: List[int], max_len: int) -> List[int]:\n",
    "    if len(ids) > max_len:\n",
    "        return ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "class EnJaDataset(Dataset):\n",
    "    def __init__(self, pairs, max_len=MAX_LEN):\n",
    "        self.data = []\n",
    "        for en, ja in pairs:\n",
    "            src = clip(encode_sentence(en, add_bos=False, add_eos=True), max_len)      # encoder gets EN + <eos>\n",
    "            tgt = clip(encode_sentence(ja, add_bos=True, add_eos=True), max_len)       # decoder: <bos> JA ... <eos>\n",
    "            # tgt_inp  = <bos> ... last token before eos\n",
    "            # tgt_out  = next tokens after bos (i.e., shifted left), includes eos\n",
    "            tgt_inp = tgt[:-1]\n",
    "            tgt_out = tgt[1:]\n",
    "            self.data.append((src, tgt_inp, tgt_out))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def pad_batch(seqs, pad_id=PAD_ID):\n",
    "    maxlen = max(len(s) for s in seqs)\n",
    "    return [s + [pad_id]*(maxlen - len(s)) for s in seqs]\n",
    "\n",
    "def collate(batch):\n",
    "    srcs, tgts_inp, tgts_out = zip(*batch)\n",
    "    srcs = pad_batch(srcs, PAD_ID)\n",
    "    tgts_inp = pad_batch(tgts_inp, PAD_ID)\n",
    "    tgts_out = pad_batch(tgts_out, PAD_ID)\n",
    "    srcs = torch.tensor(srcs, dtype=torch.long)\n",
    "    tgts_inp = torch.tensor(tgts_inp, dtype=torch.long)\n",
    "    tgts_out = torch.tensor(tgts_out, dtype=torch.long)\n",
    "    # masks: True where PAD\n",
    "    src_pad_mask = (srcs == PAD_ID)  # [B, S]\n",
    "    tgt_pad_mask = (tgts_inp == PAD_ID)  # [B, T]\n",
    "    return srcs, tgts_inp, tgts_out, src_pad_mask, tgt_pad_mask\n",
    "\n",
    "train_ds = EnJaDataset(train_pairs, MAX_LEN)\n",
    "val_ds   = EnJaDataset(val_pairs, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b00a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x: [B, L, D]\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:, :L, :]\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    # (T, T)\n",
    "    return torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, nhead: int, num_encoder_layers: int,\n",
    "                 num_decoder_layers: int, ff_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        self.tgt_tok_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=ff_dim, dropout=dropout,\n",
    "            batch_first=True  # use [B, L, D]\n",
    "        )\n",
    "        self.generator = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # initialize\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask):\n",
    "        # src: [B,S], tgt_inp: [B,T]\n",
    "        src_emb = self.pos_enc(self.src_tok_emb(src))  # [B,S,D]\n",
    "        tgt_emb = self.pos_enc(self.tgt_tok_emb(tgt_inp))  # [B,T,D]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_inp.size(1)).to(src.device)  # [T,T]\n",
    "\n",
    "        out = self.transformer(\n",
    "            src=src_emb, tgt=tgt_emb,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "        )  # [B,T,D]\n",
    "        logits = self.generator(out)  # [B,T,V]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f314fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 54,019,712\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    nhead=NHEAD,\n",
    "    num_encoder_layers=ENC_LAYERS,\n",
    "    num_decoder_layers=DEC_LAYERS,\n",
    "    ff_dim=FF_DIM,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [train]\", leave=False)\n",
    "    for src, tgt_inp, tgt_out, src_pad, tgt_pad in pbar:\n",
    "        src, tgt_inp, tgt_out = src.to(DEVICE), tgt_inp.to(DEVICE), tgt_out.to(DEVICE)\n",
    "        src_pad, tgt_pad = src_pad.to(DEVICE), tgt_pad.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(src, tgt_inp, src_pad, tgt_pad)  # [B,T,V]\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        pbar.set_postfix({\"loss\": f\"{total_loss/steps:.3f}\"})\n",
    "    return total_loss / max(1, steps)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    for src, tgt_inp, tgt_out, src_pad, tgt_pad in val_loader:\n",
    "        src, tgt_inp, tgt_out = src.to(DEVICE), tgt_inp.to(DEVICE), tgt_out.to(DEVICE)\n",
    "        src_pad, tgt_pad = src_pad.to(DEVICE), tgt_pad.to(DEVICE)\n",
    "        logits = model(src, tgt_inp, src_pad, tgt_pad)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "    return total_loss / max(1, steps)\n",
    "\n",
    "def ids_to_text(ids: List[int]) -> str:\n",
    "    # Strip padding/BOS/EOS\n",
    "    out = []\n",
    "    for i in ids:\n",
    "        if i in (PAD_ID, BOS_ID, EOS_ID):\n",
    "            continue\n",
    "        out.append(i)\n",
    "    return sp.decode(out)\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(en_text: str, max_len=MAX_LEN) -> str:\n",
    "    model.eval()\n",
    "    # Encode source\n",
    "    src = [clip(encode_sentence(en_text, add_bos=False, add_eos=True), max_len)]\n",
    "    src = torch.tensor(src, dtype=torch.long).to(DEVICE)  # [1,S]\n",
    "    src_pad = (src == PAD_ID)\n",
    "\n",
    "    # Start target with <bos>\n",
    "    tgt = torch.tensor([[BOS_ID]], dtype=torch.long).to(DEVICE)  # [1,1]\n",
    "\n",
    "    for _ in range(max_len-1):\n",
    "        tgt_pad = (tgt == PAD_ID)\n",
    "        logits = model(src, tgt, src_pad, tgt_pad)  # [1,T,V]\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # [1,1]\n",
    "        tgt = torch.cat([tgt, next_token], dim=1)  # [1,T+1]\n",
    "        if next_token.item() == EOS_ID:\n",
    "            break\n",
    "\n",
    "    return ids_to_text(tgt.squeeze(0).tolist())\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bleu(samples=256):\n",
    "    # Evaluate BLEU on a random subset of the validation set\n",
    "    if len(val_ds) == 0:\n",
    "        return 0.0\n",
    "    idxs = random.sample(range(len(val_ds)), k=min(samples, len(val_ds)))\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    for i in idxs:\n",
    "        en, ja = val_pairs[i]\n",
    "        hyp = greedy_decode(en, max_len=MAX_LEN)\n",
    "        refs.append([ja])\n",
    "        hyps.append(hyp)\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
    "    return float(bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cbacbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training… \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 6.094 | val loss 5.741 | 206.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train loss 5.622 | val loss 5.453 | 209.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train loss 5.337 | val loss 5.170 | 211.0s | BLEU 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train loss 5.121 | val loss 4.986 | 210.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train loss 4.962 | val loss 4.826 | 212.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train loss 4.841 | val loss 4.750 | 212.3s | BLEU 2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train loss 4.728 | val loss 4.623 | 209.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train loss 4.623 | val loss 4.513 | 208.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | train loss 4.534 | val loss 4.431 | 212.3s | BLEU 1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss 4.463 | val loss 4.384 | 208.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train loss 4.404 | val loss 4.336 | 208.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | train loss 4.356 | val loss 4.297 | 209.1s | BLEU 4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | train loss 4.307 | val loss 4.270 | 209.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | train loss 4.271 | val loss 4.224 | 209.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | train loss 4.233 | val loss 4.190 | 213.3s | BLEU 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | train loss 4.205 | val loss 4.180 | 213.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | train loss 4.173 | val loss 4.144 | 213.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | train loss 4.149 | val loss 4.122 | 213.1s | BLEU 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | train loss 4.127 | val loss 4.131 | 213.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | train loss 4.105 | val loss 4.088 | 213.4s | BLEU 1.8\n",
      "\n",
      "Training complete.\n",
      "Best BLEU (val): 4.955349584429408\n",
      "Saved best model to: artifacts\\enja_transformer.pt\n",
      "Tokenizer model: artifacts\\spm_enja.model\n"
     ]
    }
   ],
   "source": [
    "best_val = float(\"inf\")\n",
    "best_bleu = 0.0\n",
    "ckpt_path = ARTIFACTS_DIR / \"enja_transformer.pt\"\n",
    "spm_model_path = ARTIFACTS_DIR / \"spm_enja.model\"\n",
    "\n",
    "print(\"Starting training… \")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(epoch)\n",
    "    val_loss = evaluate_loss()\n",
    "    took = time.time() - t0\n",
    "\n",
    "    msg = f\"Epoch {epoch:02d} | train loss {tr_loss:.3f} | val loss {val_loss:.3f} | {took:.1f}s\"\n",
    "\n",
    "    # Optionally compute BLEU every few epochs\n",
    "    if epoch % max(1, EPOCHS // 6) == 0 or epoch == EPOCHS:\n",
    "        bleu = compute_bleu(samples=128)\n",
    "        msg += f\" | BLEU {bleu:.1f}\"\n",
    "        if bleu > best_bleu:\n",
    "            best_bleu = bleu\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"config\": {\n",
    "                    \"VOCAB_SIZE\": VOCAB_SIZE, \"EMBED_DIM\": EMBED_DIM, \"FF_DIM\": FF_DIM,\n",
    "                    \"NHEAD\": NHEAD, \"ENC_LAYERS\": ENC_LAYERS, \"DEC_LAYERS\": DEC_LAYERS,\n",
    "                    \"DROPOUT\": DROPOUT, \"MAX_LEN\": MAX_LEN\n",
    "                }\n",
    "            }, ckpt_path)\n",
    "\n",
    "    print(msg)\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"Best BLEU (val):\", best_bleu)\n",
    "print(\"Saved best model to:\", ckpt_path)\n",
    "print(\"Tokenizer model:\", spm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c25bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EN: hello, how are you?\n",
      "JA: 元気?\n",
      "\n",
      "EN: what time is the last train?\n",
      "JA: 電車はどのくらいですか。\n",
      "\n",
      "EN: my dog likes to bark.\n",
      "JA: 犬が吠えで吠えそう。\n",
      "\n",
      "EN: what did you eat for breakfast?\n",
      "JA: 朝食は何を食べましたか。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training results:\n",
    "examples = [\n",
    "    \"hello, how are you?\",\n",
    "    \"what time is the last train?\",\n",
    "    \"my dog likes to bark.\",\n",
    "    \"what did you eat for breakfast?\"\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(\"\\nEN:\", s)\n",
    "    print(\"JA:\", greedy_decode(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db4b5077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final model to: artifacts\\enja_transformer_final.pt\n",
      "Tokenizer model already at: artifacts\\spm_enja.model\n"
     ]
    }
   ],
   "source": [
    "# To save training artifacts\n",
    "final_ckpt = ARTIFACTS_DIR / \"enja_transformer_final.pt\"\n",
    "final_spm = ARTIFACTS_DIR / \"spm_enja.model\"\n",
    "\n",
    "# To save model checkpoint (weights + config)\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"config\": {\n",
    "        \"VOCAB_SIZE\": VOCAB_SIZE, \"EMBED_DIM\": EMBED_DIM, \"FF_DIM\": FF_DIM,\n",
    "        \"NHEAD\": NHEAD, \"ENC_LAYERS\": ENC_LAYERS, \"DEC_LAYERS\": DEC_LAYERS,\n",
    "        \"DROPOUT\": DROPOUT, \"MAX_LEN\": MAX_LEN\n",
    "    }\n",
    "}, final_ckpt)\n",
    "\n",
    "print(\"Saved final model to:\", final_ckpt)\n",
    "print(\"Tokenizer model already at:\", final_spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bf357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentencePiece tokenizer\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(str(ARTIFACTS_DIR / \"spm_enja.model\"))\n",
    "\n",
    "# Reload model\n",
    "ckpt = torch.load(ARTIFACTS_DIR / \"enja_transformer_final.pt\", map_location=DEVICE)\n",
    "cfg = ckpt[\"config\"]\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=cfg[\"VOCAB_SIZE\"],\n",
    "    embed_dim=cfg[\"EMBED_DIM\"],\n",
    "    nhead=cfg[\"NHEAD\"],\n",
    "    num_encoder_layers=cfg[\"ENC_LAYERS\"],\n",
    "    num_decoder_layers=cfg[\"DEC_LAYERS\"],\n",
    "    ff_dim=cfg[\"FF_DIM\"],\n",
    "    dropout=cfg[\"DROPOUT\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model + tokenizer loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
